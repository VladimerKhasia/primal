{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa68b0a",
   "metadata": {},
   "source": [
    "Unlike the paper, the code calculates primes dynamically instead of using a lookup table.\n",
    "This code generates Figure 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-distribution landscape and rms error combined\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# 0. SETUP & SEEDING\n",
    "# ==========================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. MODEL DEFINITIONS\n",
    "# ==========================================\n",
    "\n",
    "def get_first_n_primes(n):\n",
    "    if n < 1: return torch.tensor([], device=device)\n",
    "    limit = int(n * (math.log(n) + math.log(math.log(n)))) + 20 if n > 5 else 20\n",
    "    sieve = torch.ones(limit, dtype=torch.bool, device=device)\n",
    "    sieve[0:2] = False\n",
    "    for i in range(2, int(math.isqrt(limit)) + 1):\n",
    "        if sieve[i]: sieve[i*i : limit : i] = False\n",
    "    primes = torch.nonzero(sieve).flatten()\n",
    "    while len(primes) < n:\n",
    "        limit *= 2\n",
    "        sieve = torch.ones(limit, dtype=torch.bool, device=device)\n",
    "        sieve[0:2] = False\n",
    "        for i in range(2, int(math.isqrt(limit)) + 1):\n",
    "            if sieve[i]: sieve[i*i : limit : i] = False\n",
    "        primes = torch.nonzero(sieve).flatten()\n",
    "    return primes[:n].float()\n",
    "\n",
    "class Method_RandomGaussian(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, n_seq):\n",
    "        return torch.nn.functional.normalize(torch.randn(n_seq, self.dim, device=device), p=2, dim=1)\n",
    "\n",
    "# class StaticPrime(nn.Module):\n",
    "#     def __init__(self, dim):\n",
    "#         super().__init__()\n",
    "#         primes = get_first_n_primes(dim // 2)\n",
    "#         self.register_buffer(\"freqs\", (torch.sqrt(primes))) \n",
    "\n",
    "#     def forward(self, n_seq):\n",
    "#         t = torch.arange(n_seq, device=self.freqs.device).double()\n",
    "#         freqs_double = self.freqs.double()\n",
    "#         angles = torch.outer(t, freqs_double) * 2 * math.pi\n",
    "#         return torch.cat([angles.cos().float(), angles.sin().float()], dim=-1)\n",
    "\n",
    "# def get_models(d):\n",
    "#     return {\n",
    "#         \"RandNorm Gaussian\": Method_RandomGaussian(d),\n",
    "#         \"Static Prime\": StaticPrime(d),\n",
    "#     }\n",
    "\n",
    "class StaticPrime(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        primes = get_first_n_primes(dim // 2)                                       \n",
    "        self.register_buffer(\"freqs\", (torch.sqrt(primes)))\n",
    "    def forward(self, n_seq):\n",
    "        t = torch.arange(n_seq, device=self.freqs.device)\n",
    "        angles = torch.outer(t, self.freqs) * 2 * math.pi\n",
    "        return torch.cat([angles.cos(), angles.sin()], dim=-1)\n",
    "        \n",
    "# ==========================================\n",
    "# 2. DATA GENERATION & METRIC CALCULATION\n",
    "# ==========================================\n",
    "\n",
    "def calculate_tightness_score(n_seq=10000, d_vals=[16, 64, 256, 1024, 4096]):\n",
    "    \"\"\"\n",
    "    Calculates the 'Tightness' (RMS of cross-correlations) for comparison text.\n",
    "    Lower RMS is better.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    print(\"Calculating Comparative Scores...\")\n",
    "    \n",
    "    # We aggregate the score across multiple dimensions to get a robust average\n",
    "    for name in [\"RandNorm Gaussian\", \"Static Prime\"]:\n",
    "        method_scores = []\n",
    "        for d in d_vals:\n",
    "            model = get_models(d)[name].to(device)\n",
    "            with torch.no_grad():\n",
    "                vecs = model(n_seq)\n",
    "                vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)\n",
    "                gram = torch.mm(vecs, vecs.t())\n",
    "                mask = ~torch.eye(n_seq, device=device, dtype=torch.bool)\n",
    "                vals = gram[mask]\n",
    "                # RMS Calculation\n",
    "                rms = torch.sqrt(torch.mean(vals**2)).item()\n",
    "                method_scores.append(rms)\n",
    "        scores[name] = np.mean(method_scores)\n",
    "    \n",
    "    baseline = scores[\"RandNorm Gaussian\"]\n",
    "    ours = scores[\"Static Prime\"]\n",
    "    \n",
    "    # Calculate % improvement (Reduction in RMS error)\n",
    "    improvement = ((baseline - ours) / baseline) * 100\n",
    "    return scores, improvement\n",
    "\n",
    "def generate_distribution_data():\n",
    "    \"\"\"Row 1 Data: Log-Distribution\"\"\"\n",
    "    print(\"Generating Distribution Data...\")\n",
    "    fixed_N = 10000 \n",
    "    d_values = [16, 64, 256, 1024, 4096] \n",
    "    bins = 100\n",
    "    hist_range = (-0.4, 0.4)\n",
    "    all_data = []\n",
    "\n",
    "    for d in d_values:\n",
    "        models = get_models(d)\n",
    "        for name, model in models.items():\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                vecs = model(fixed_N)\n",
    "                vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)\n",
    "                gram = torch.mm(vecs, vecs.t())\n",
    "                mask = ~torch.eye(fixed_N, device=device, dtype=torch.bool)\n",
    "                vals = gram[mask]\n",
    "                \n",
    "                hist = torch.histc(vals, bins=bins, min=hist_range[0], max=hist_range[1])\n",
    "                hist_cpu = hist.cpu().numpy()\n",
    "                hist_density = hist_cpu / vals.numel()\n",
    "                hist_log = np.log10(hist_density + 1e-9)\n",
    "                \n",
    "                bin_edges = np.linspace(hist_range[0], hist_range[1], bins + 1)\n",
    "                bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "                \n",
    "                for b_idx, val in enumerate(hist_log):\n",
    "                    all_data.append({\n",
    "                        \"Method\": name,\n",
    "                        \"Dimension\": d,\n",
    "                        \"Similarity\": bin_centers[b_idx],\n",
    "                        \"LogDensity\": val\n",
    "                    })\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def generate_rms_surface_data():\n",
    "    \"\"\"Row 2 Data: RMS Error Surface\"\"\"\n",
    "    print(\"Generating RMS Surface Data...\")\n",
    "    n_values = [100, 1000, 5000]\n",
    "    d_values = [16, 64, 128, 256, 512, 1024]\n",
    "    results = []\n",
    "\n",
    "    for d in d_values:\n",
    "        models = get_models(d)\n",
    "        for n in n_values:\n",
    "            for name, model in models.items():\n",
    "                model.to(device)\n",
    "                with torch.no_grad():\n",
    "                    vecs = model(n)\n",
    "                    vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)\n",
    "                    gram = torch.mm(vecs, vecs.t())\n",
    "                    mask = ~torch.eye(n, device=device, dtype=torch.bool)\n",
    "                    off_diag = gram[mask]\n",
    "                    rms_error = torch.sqrt(torch.mean(off_diag**2)).item()\n",
    "                \n",
    "                results.append({\n",
    "                    \"Method\": name,\n",
    "                    \"N\": n,\n",
    "                    \"Dimension\": d,\n",
    "                    \"RMS\": rms_error\n",
    "                })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ==========================================\n",
    "# 3. VISUALIZATION PIPELINE\n",
    "# ==========================================\n",
    "\n",
    "def plot_comparative_dashboard():\n",
    "    # 1. Compute Metrics\n",
    "    scores, tightness_improvement = calculate_tightness_score()\n",
    "    \n",
    "    # 2. Get Plotting Data\n",
    "    df_dist = generate_distribution_data()\n",
    "    df_rms = generate_rms_surface_data()\n",
    "\n",
    "    # 3. Setup Figure (2 Rows x 2 Cols)\n",
    "    fig = plt.figure(figsize=(18, 12)) \n",
    "    \n",
    "    methods = [\"RandNorm Gaussian\", \"Static Prime\"]\n",
    "    titles = [\"Baseline (Gaussian)\", \"Ours (StaticPrime)\"]\n",
    "    \n",
    "    # --- ROW 1: DISTRIBUTION (3D) ---\n",
    "    z_min_dist = df_dist[\"LogDensity\"].min()\n",
    "    z_max_dist = df_dist[\"LogDensity\"].max()\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        ax = fig.add_subplot(2, 2, i+1, projection='3d')\n",
    "        subset = df_dist[df_dist[\"Method\"] == method]\n",
    "        \n",
    "        x = subset[\"Similarity\"].values\n",
    "        y = subset[\"Dimension\"].values\n",
    "        z = subset[\"LogDensity\"].values\n",
    "        \n",
    "        xi = np.linspace(x.min(), x.max(), 60)\n",
    "        yi = np.linspace(y.min(), y.max(), 60)\n",
    "        Xi, Yi = np.meshgrid(xi, yi)\n",
    "        Zi = griddata((x, y), z, (Xi, Yi), method='cubic')\n",
    "        \n",
    "        ax.plot_surface(Xi, Yi, Zi, cmap='plasma', linewidth=0, antialiased=True, alpha=0.9)\n",
    "        \n",
    "        ax.set_title(f\"{titles[i]}\", fontsize=15, weight='bold')\n",
    "        ax.set_xlabel('Cosine Similarity')\n",
    "        ax.set_ylabel('Dimension (d)')\n",
    "        ax.set_zlabel('Log Density')\n",
    "        ax.set_zlim(z_min_dist, z_max_dist)\n",
    "        ax.view_init(elev=30, azim=-60)\n",
    "        ax.grid(False)\n",
    "        ax.xaxis.pane.fill = ax.yaxis.pane.fill = ax.zaxis.pane.fill = False\n",
    "\n",
    "    # --- ADD CENTRAL ANNOTATION FOR ROW 1 ---\n",
    "    # Calculates relative improvements to display between plots\n",
    "    fig.text(0.5, 0.75, \n",
    "             f\"DISTRIBUTION TIGHTNESS\\n(RMS Reduction)\\n\\nOurs is\\n{tightness_improvement:.1f}%\\nBetter\\n➜\", \n",
    "             ha='center', va='center', fontsize=14, weight='bold', color='darkred',\n",
    "             bbox=dict(boxstyle=\"rarrow,pad=0.3\", fc=\"white\", ec=\"darkred\", lw=2))\n",
    "\n",
    "    # --- ROW 2: RMS ERROR SURFACE (3D) ---\n",
    "    z_min_rms = df_rms[\"RMS\"].min()\n",
    "    z_max_rms = df_rms[\"RMS\"].max()\n",
    "    \n",
    "    rms_improvements = {} # To calculate Row 2 improvement\n",
    "\n",
    "    for i, method in enumerate(methods):\n",
    "        ax = fig.add_subplot(2, 2, i+3, projection='3d')\n",
    "        subset = df_rms[df_rms[\"Method\"] == method]\n",
    "        \n",
    "        x = subset[\"Dimension\"].values\n",
    "        y = subset[\"N\"].values\n",
    "        z = subset[\"RMS\"].values\n",
    "        \n",
    "        xi = np.linspace(x.min(), x.max(), 60)\n",
    "        yi = np.linspace(y.min(), y.max(), 60)\n",
    "        Xi, Yi = np.meshgrid(xi, yi)\n",
    "        Zi = griddata((x, y), z, (Xi, Yi), method='cubic')\n",
    "        \n",
    "        norm = plt.Normalize(z_min_rms, z_max_rms)\n",
    "        ax.plot_surface(Xi, Yi, Zi, cmap='coolwarm', norm=norm, linewidth=0, antialiased=True, alpha=0.9)\n",
    "        \n",
    "        avg_rms = subset[\"RMS\"].mean()\n",
    "        rms_improvements[method] = avg_rms # Store for calc\n",
    "        \n",
    "        ax.set_title(f\"{titles[i]}\\nAvg RMS Error: {avg_rms:.4f}\", fontsize=15, weight='bold')\n",
    "        ax.set_xlabel('Dimension (d)')\n",
    "        ax.set_ylabel('Seq Length (N)')\n",
    "        ax.set_zlabel('RMS Error')\n",
    "        ax.set_zlim(z_min_rms, z_max_rms)\n",
    "        ax.view_init(elev=30, azim=-130)\n",
    "        ax.grid(False)\n",
    "        ax.xaxis.pane.fill = ax.yaxis.pane.fill = ax.zaxis.pane.fill = False\n",
    "\n",
    "    base_rms = rms_improvements[\"RandNorm Gaussian\"]\n",
    "    our_rms = rms_improvements[\"Static Prime\"]\n",
    "    rms_imp = ((base_rms - our_rms) / base_rms) * 100\n",
    "    \n",
    "    fig.text(0.5, 0.25, \n",
    "             f\"GLOBAL ERROR\\n(Lower is Better)\\n\\nOurs is\\n{rms_imp:.1f}%\\nLower\\n➜\", \n",
    "             ha='center', va='center', fontsize=14, weight='bold', color='navy',\n",
    "             bbox=dict(boxstyle=\"rarrow,pad=0.3\", fc=\"white\", ec=\"navy\", lw=2))\n",
    "\n",
    "    plt.subplots_adjust(top=0.88, wspace=0.3, hspace=0.3)\n",
    "    plt.suptitle(f\"Comparison: Gaussian Baseline vs. Static Prime (Ours)\\nAnalyzed over N=10,000 samples\", \n",
    "                 fontsize=22, weight='bold', y=0.98)\n",
    "    \n",
    "    print(f\"Rendering Plot... (Tightness Improvement: {tightness_improvement:.2f}%)\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_comparative_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
