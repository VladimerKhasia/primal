{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550df532",
   "metadata": {},
   "source": [
    "Unlike the paper, the code calculates primes dynamically instead of using a lookup table.\n",
    "This code generates Figure 2 and all details behind figure 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code contains static version of our method represented in class StaticPrime and the experiments showing its advantages.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats.qmc import Sobol\n",
    "from scipy.interpolate import griddata\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    Seeds all sources of randomness to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Ensure deterministic behavior for cuDNN\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(f\"Global seed set to: {seed}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. HELPER FUNCTIONS & METHODS\n",
    "# ==========================================\n",
    "\n",
    "def get_first_n_primes(n):\n",
    "    if n < 1: return torch.tensor([], device=device)\n",
    "    limit = int(n * (math.log(n) + math.log(math.log(n)))) + 20 if n > 5 else 20\n",
    "    sieve = torch.ones(limit, dtype=torch.bool, device=device)\n",
    "    sieve[0:2] = False\n",
    "    for i in range(2, int(math.isqrt(limit)) + 1):\n",
    "        if sieve[i]: sieve[i*i : limit : i] = False\n",
    "    primes = torch.nonzero(sieve).flatten()\n",
    "    while len(primes) < n:\n",
    "        limit *= 2\n",
    "        sieve = torch.ones(limit, dtype=torch.bool, device=device)\n",
    "        sieve[0:2] = False\n",
    "        for i in range(2, int(math.isqrt(limit)) + 1):\n",
    "            if sieve[i]: sieve[i*i : limit : i] = False\n",
    "        primes = torch.nonzero(sieve).flatten()\n",
    "    return primes[:n].float()\n",
    "\n",
    "def get_welch_bound(N, d):\n",
    "    if N <= d: return 0.0\n",
    "    return math.sqrt((N - d) / (d * (N - 1)))\n",
    "\n",
    "class Method_RandomGaussian(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, n_seq):\n",
    "        # Generates (N, d) matrix and normalizes rows (dim=1) to unit vectors\n",
    "        # Note: We use dim=1 because your code structure is (Sequence_Length, Dimension)\n",
    "        return torch.nn.functional.normalize(torch.randn(n_seq, self.dim, device=device), p=2, dim=1)\n",
    "\n",
    "class StaticPrime(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        primes = get_first_n_primes(dim // 2)                                       \n",
    "        self.register_buffer(\"freqs\", (torch.sqrt(primes)))\n",
    "    def forward(self, n_seq):\n",
    "        t = torch.arange(n_seq, device=self.freqs.device)\n",
    "        angles = torch.outer(t, self.freqs) * 2 * math.pi\n",
    "        return torch.cat([angles.cos(), angles.sin()], dim=-1)\n",
    "\n",
    "# class StaticPrime(nn.Module):\n",
    "#     def __init__(self, dim):\n",
    "#         super().__init__()\n",
    "#         primes = get_first_n_primes(dim // 2)\n",
    "#         # Store as float32 to save memory, but cast later\n",
    "#         self.register_buffer(\"freqs\", (torch.sqrt(primes))) \n",
    "\n",
    "#     def forward(self, n_seq):\n",
    "#         t = torch.arange(n_seq, device=self.freqs.device).double() # Cast time to double\n",
    "#         freqs_double = self.freqs.double() # Cast freqs to double\n",
    "        \n",
    "#         # High precision multiplication\n",
    "#         angles = torch.outer(t, freqs_double) * 2 * math.pi\n",
    "        \n",
    "#         # Cast back to float for output\n",
    "#         return torch.cat([angles.cos().float(), angles.sin().float()], dim=-1)\n",
    "        \n",
    "# Dictionary of methods\n",
    "def get_models(d):\n",
    "    return {\n",
    "        \"RandNorm Gaussian\": Method_RandomGaussian(d),\n",
    "        \"Static Prime\": StaticPrime(d),\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# 1. EXPERIMENT 1: DISTRIBUTION (HASHING QUALITY)\n",
    "# ==========================================\n",
    "\n",
    "def run_exp2_distribution_3d():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING EXPERIMENT 1: 3D Distribution Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # We fix N to a large number to see the distribution clearly\n",
    "    # varying d to see how the 'spike' sharpens\n",
    "    fixed_N = 10000 \n",
    "    d_values = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "    \n",
    "    bins = 100\n",
    "    # We focus on the center -0.4 to 0.4 because that's where the \"Needle\" is\n",
    "    hist_range = (-0.4, 0.4) \n",
    "    \n",
    "    all_data = []\n",
    "    tightness_metrics = []\n",
    "\n",
    "    for d in d_values:\n",
    "        models = get_models(d)\n",
    "        print(f\"Processing Distribution for d={d}...\")\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                vecs = model(fixed_N)\n",
    "                # Normalize\n",
    "                vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)\n",
    "                \n",
    "                # Calculate Gram Matrix (in chunks to avoid OOM if necessary)\n",
    "                # For N=10000, float32, matrix is ~400MB, safe for modern GPU\n",
    "                gram = torch.mm(vecs, vecs.t())\n",
    "                \n",
    "                # Mask diagonal (self-correlation is always 1.0)\n",
    "                mask = ~torch.eye(fixed_N, device=device, dtype=torch.bool)\n",
    "                vals = gram[mask]\n",
    "                # Calculate Root Mean Square (RMS) of the correlations.\n",
    "                # A lower RMS indicates the distribution is tighter around 0.\n",
    "                rms_val = torch.sqrt(torch.mean(vals**2)).item()\n",
    "                tightness_metrics.append({\n",
    "                    \"Method\": name,\n",
    "                    \"Dimension\": d,\n",
    "                    \"RMS\": rms_val\n",
    "                })                \n",
    "                # Compute Histogram on GPU\n",
    "                hist = torch.histc(vals, bins=bins, min=hist_range[0], max=hist_range[1])\n",
    "                hist_cpu = hist.cpu().numpy()\n",
    "                \n",
    "                # Convert to density\n",
    "                hist_density = hist_cpu / vals.numel()\n",
    "                \n",
    "                # Logarithm (avoid log(0))\n",
    "                hist_log = np.log10(hist_density + 1e-9)\n",
    "                \n",
    "                # Generate Bin Centers\n",
    "                bin_edges = np.linspace(hist_range[0], hist_range[1], bins + 1)\n",
    "                bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "                \n",
    "                for b_idx, val in enumerate(hist_log):\n",
    "                    all_data.append({\n",
    "                        \"Method\": name,\n",
    "                        \"Dimension\": d,\n",
    "                        \"Similarity\": bin_centers[b_idx],\n",
    "                        \"LogDensity\": val\n",
    "                    })\n",
    "                    \n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DISTRIBUTION TIGHTNESS ANALYSIS\")\n",
    "    print(\"Calculation Method: Root Mean Square (RMS) of cross-correlations.\")\n",
    "    print(\"Lower RMS values = Tighter distribution (better orthogonality).\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    stats_df = pd.DataFrame(tightness_metrics)\n",
    "    \n",
    "    # Calculate the average RMS across all dimensions for each method\n",
    "    summary = stats_df.groupby(\"Method\")[\"RMS\"].mean().sort_values()\n",
    "    print(summary)\n",
    "    \n",
    "    # detailed comparison if we have exactly 2 methods active\n",
    "    methods_list = summary.index.tolist()\n",
    "    if len(methods_list) == 2:\n",
    "        best_method = methods_list[0] # Sorted ascending, so first is lowest (best)\n",
    "        worst_method = methods_list[1]\n",
    "        val_best = summary[best_method]\n",
    "        val_worst = summary[worst_method]\n",
    "        \n",
    "        # Calculate percentage reduction\n",
    "        reduction = ((val_worst - val_best) / val_worst) * 100\n",
    "        \n",
    "        print(\"\\nCONCLUSION:\")\n",
    "        print(f\"The values for '{best_method}' are {reduction:.2f}% lower\")\n",
    "        print(f\"than '{worst_method}' on average, indicating a tighter distribution.\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "    # --- PLOTTING ---\n",
    "    methods = df[\"Method\"].unique()\n",
    "    \n",
    "    for method in methods:\n",
    "        subset = df[df[\"Method\"] == method]\n",
    "        \n",
    "        # Prepare Grid for Interpolation\n",
    "        x = subset[\"Similarity\"].values\n",
    "        y = subset[\"Dimension\"].values\n",
    "        z = subset[\"LogDensity\"].values\n",
    "        \n",
    "        # Create dense mesh\n",
    "        xi = np.linspace(x.min(), x.max(), 100)\n",
    "        yi = np.linspace(y.min(), y.max(), 100) # Linear interpolation of D\n",
    "        Xi, Yi = np.meshgrid(xi, yi)\n",
    "        Zi = griddata((x, y), z, (Xi, Yi), method='cubic')\n",
    "        \n",
    "        # Plot\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        surf = ax.plot_surface(Xi, Yi, Zi, cmap='plasma', \n",
    "                               linewidth=0, antialiased=True, rcount=100, ccount=100, alpha=0.9)\n",
    "        \n",
    "        ax.set_title(f\"EXP 1: Log-Distribution Landscape - {method}\\n(N={fixed_N})\", fontsize=15, weight='bold')\n",
    "        ax.set_xlabel('\\nCosine Similarity', fontsize=11)\n",
    "        ax.set_ylabel('\\nDimension (d)', fontsize=11)\n",
    "        ax.set_zlabel('\\nLog Density', fontsize=11)\n",
    "        \n",
    "        # Remove Panes\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.grid(False)\n",
    "        \n",
    "        ax.view_init(elev=30, azim=-60)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 2. EXPERIMENT 2: WELCH OPTIMALITY (A & B)\n",
    "# ==========================================\n",
    "\n",
    "def run_exp3_welch_3d():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING EXPERIMENT 2: Welch Optimality Grid Search\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    n_values = [100, 1000, 10000]\n",
    "    d_values = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "    results = []\n",
    "    \n",
    "    total_steps = len(n_values) * len(d_values)\n",
    "    step = 0\n",
    "    \n",
    "    for d in d_values:\n",
    "        models = get_models(d)\n",
    "        for n in n_values:\n",
    "            wb = get_welch_bound(n, d)\n",
    "            \n",
    "            # If N <= d, Welch Bound is 0 mathematically, but practically coherence exists.\n",
    "            # We handle this in plotting.\n",
    "            \n",
    "            for name, model in models.items():\n",
    "                model.to(device)\n",
    "                with torch.no_grad():\n",
    "                    vecs = model(n)\n",
    "                    # Calculating Max Coherence\n",
    "                    vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)\n",
    "                    gram = torch.mm(vecs, vecs.t())\n",
    "                    mask = ~torch.eye(n, device=device, dtype=torch.bool)\n",
    "                    vals = gram[mask]\n",
    "                    \n",
    "                    if vals.numel() > 0:\n",
    "                        max_coh = vals.abs().max().item()\n",
    "                    else:\n",
    "                        max_coh = 0.0\n",
    "                \n",
    "                ratio = max_coh / wb if wb > 0 else np.nan # Avoid div by zero\n",
    "                \n",
    "                results.append({\n",
    "                    \"Method\": name,\n",
    "                    \"N\": n,\n",
    "                    \"Dimension\": d,\n",
    "                    \"MaxCoh\": max_coh,\n",
    "                    \"WelchBound\": wb,\n",
    "                    \"Optimality\": ratio\n",
    "                })\n",
    "            \n",
    "            step += 1\n",
    "            print(f\"\\rProcessing Grid: {step}/{total_steps}\", end=\"\")\n",
    "            \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    methods = df[\"Method\"].unique()\n",
    "    \n",
    "    # --- PLOT A: Raw Max Coherence + Welch Bound Surface ---\n",
    "    print(\"\\nGenerating Plot A (Raw Coherence vs Bound)...\")\n",
    "    for method in methods:\n",
    "        subset = df[df[\"Method\"] == method]\n",
    "        \n",
    "        # Data for Method\n",
    "        x = subset[\"Dimension\"].values\n",
    "        y = subset[\"N\"].values\n",
    "        z_method = subset[\"MaxCoh\"].values\n",
    "        z_bound = subset[\"WelchBound\"].values\n",
    "        \n",
    "        # Interpolation Grid\n",
    "        xi = np.linspace(x.min(), x.max(), 100)\n",
    "        yi = np.linspace(y.min(), y.max(), 100)\n",
    "        Xi, Yi = np.meshgrid(xi, yi)\n",
    "        \n",
    "        # Interpolate both Method and Bound\n",
    "        Zi_method = griddata((x, y), z_method, (Xi, Yi), method='cubic')\n",
    "        Zi_bound = griddata((x, y), z_bound, (Xi, Yi), method='cubic')\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 9))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        # 1. Plot Method Surface (Solid, Colorful)\n",
    "        surf = ax.plot_surface(Xi, Yi, Zi_method, cmap='viridis', \n",
    "                               linewidth=0, antialiased=True, alpha=0.9, label=method)\n",
    "        \n",
    "        # 2. Plot Welch Bound Surface (Wireframe, Black/Grey, Translucent)\n",
    "        # We use wireframe so it doesn't obscure the method completely\n",
    "        ax.plot_wireframe(Xi, Yi, Zi_bound, color='black', alpha=0.4, \n",
    "                          rstride=5, cstride=5, linewidth=0.8, label=\"Welch Bound\")\n",
    "        \n",
    "        ax.set_title(f\"EXP 2 - PLOT A: {method}\\nMax Coherence vs Welch Bound (Lower is Better)\", fontsize=15)\n",
    "        ax.set_xlabel('\\nDimension (d)', fontsize=11)\n",
    "        ax.set_ylabel('\\nSequence Length (N)', fontsize=11)\n",
    "        ax.set_zlabel('\\nMax Coherence', fontsize=11)\n",
    "        \n",
    "        # Clean look\n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.grid(False)\n",
    "        \n",
    "        # Add legend proxy (Wireframe doesn't auto-legend well in 3D)\n",
    "        fake2Dline = plt.Line2D([0],[0], linestyle=\"none\", c='black', marker='o')\n",
    "        ax.legend([fake2Dline], ['Welch Bound Surface'], numpoints = 1)\n",
    "\n",
    "        ax.view_init(elev=25, azim=-130)\n",
    "        plt.show()\n",
    "\n",
    "    # --- PLOT B: Optimality Ratio (Actual / Bound) ---\n",
    "    print(\"\\nGenerating Plot B (Optimality Ratio)...\")\n",
    "    for method in methods:\n",
    "        subset = df[df[\"Method\"] == method]\n",
    "        \n",
    "        # Filter out NaNs (where N < d) for cleaner interpolation\n",
    "        subset = subset.dropna(subset=['Optimality'])\n",
    "        \n",
    "        x = subset[\"Dimension\"].values\n",
    "        y = subset[\"N\"].values\n",
    "        z = subset[\"Optimality\"].values\n",
    "        \n",
    "        # Interpolation\n",
    "        xi = np.linspace(x.min(), x.max(), 100)\n",
    "        yi = np.linspace(y.min(), y.max(), 100)\n",
    "        Xi, Yi = np.meshgrid(xi, yi)\n",
    "        Zi = griddata((x, y), z, (Xi, Yi), method='cubic')\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 9))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        # Plot Surface\n",
    "        surf = ax.plot_surface(Xi, Yi, Zi, cmap='inferno', \n",
    "                               linewidth=0, antialiased=True, rcount=100, ccount=100, alpha=0.9)\n",
    "        \n",
    "        # Add a flat plane at Z=1.0 (Physical Limit)\n",
    "        plane = np.ones_like(Zi)\n",
    "        ax.plot_surface(Xi, Yi, plane, color='cyan', alpha=0.2)\n",
    "        \n",
    "        ax.set_title(f\"EXP 2 - PLOT B: {method}\\nOptimality Ratio (1.0 = Perfect/Theoretical Limit)\", fontsize=15)\n",
    "        ax.set_xlabel('\\nDimension (d)', fontsize=11)\n",
    "        ax.set_ylabel('\\nSequence Length (N)', fontsize=11)\n",
    "        ax.set_zlabel('\\nRatio (Actual / Bound)', fontsize=11)\n",
    "        \n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.grid(False)\n",
    "        \n",
    "        cbar = fig.colorbar(surf, ax=ax, shrink=0.5, aspect=12, pad=0.1)\n",
    "        cbar.set_label('Optimality Ratio')\n",
    "        \n",
    "        ax.view_init(elev=25, azim=-130)\n",
    "        plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 3. EXPERIMENT 3: RMS ERROR SURFACE\n",
    "# ==========================================\n",
    "\n",
    "def run_exp4_rms_error_3d():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING EXPERIMENT 3: RMS Error Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Grid definition\n",
    "    # We use a range of N and d similar to previous experiments\n",
    "    n_values = [100, 500, 1000, 2500, 5000]\n",
    "    d_values = [16, 32, 64, 128, 256, 512, 1024]\n",
    "    \n",
    "    results = []\n",
    "    total_steps = len(n_values) * len(d_values)\n",
    "    step = 0\n",
    "    \n",
    "    print(\"Calculating RMS Errors...\")\n",
    "    \n",
    "    for d in d_values:\n",
    "        models = get_models(d)\n",
    "        for n in n_values:\n",
    "            \n",
    "            for name, model in models.items():\n",
    "                model.to(device)\n",
    "                with torch.no_grad():\n",
    "                    vecs = model(n)\n",
    "                    vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)\n",
    "                    \n",
    "                    # Compute Gram Matrix\n",
    "                    gram = torch.mm(vecs, vecs.t())\n",
    "                    \n",
    "                    # Mask the diagonal (we only care about cross-correlations)\n",
    "                    mask = ~torch.eye(n, device=device, dtype=torch.bool)\n",
    "                    off_diag = gram[mask]\n",
    "                    \n",
    "                    # Calculate RMS (Root Mean Square) of off-diagonal elements\n",
    "                    # RMS = sqrt(mean(x^2))\n",
    "                    rms_error = torch.sqrt(torch.mean(off_diag**2)).item()\n",
    "                \n",
    "                results.append({\n",
    "                    \"Method\": name,\n",
    "                    \"N\": n,\n",
    "                    \"Dimension\": d,\n",
    "                    \"RMS\": rms_error\n",
    "                })\n",
    "            \n",
    "            step += 1\n",
    "            print(f\"\\rProgress: {step}/{total_steps}\", end=\"\")\n",
    "            \n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nPlotting results...\")\n",
    "\n",
    "    # --- PLOTTING ---\n",
    "    methods = df[\"Method\"].unique()\n",
    "    \n",
    "    # Determine Global Z-Limits (Keep this from previous step)\n",
    "    global_z_min = df[\"RMS\"].min()\n",
    "    global_z_max = df[\"RMS\"].max()\n",
    "    z_margin = (global_z_max - global_z_min) * 0.1\n",
    "    z_lims = (max(0, global_z_min - z_margin), global_z_max + z_margin)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    for method in methods:\n",
    "        subset = df[df[\"Method\"] == method]\n",
    "        \n",
    "        overall_avg_rms = subset[\"RMS\"].mean()\n",
    "        print(f\"Method: {method:15s} | Overall Avg RMS: {overall_avg_rms:.6f}\")\n",
    "        \n",
    "        x = subset[\"Dimension\"].values\n",
    "        y = subset[\"N\"].values\n",
    "        z = subset[\"RMS\"].values\n",
    "        \n",
    "        # Interpolation Grid\n",
    "        xi = np.linspace(x.min(), x.max(), 100)\n",
    "        yi = np.linspace(y.min(), y.max(), 100)\n",
    "        Xi, Yi = np.meshgrid(xi, yi)\n",
    "        Zi = griddata((x, y), z, (Xi, Yi), method='cubic')\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 9))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        surf = ax.plot_surface(Xi, Yi, Zi, cmap='coolwarm', \n",
    "                               linewidth=0, antialiased=True, rcount=100, ccount=100, alpha=0.9)\n",
    "        \n",
    "        # ==========================================\n",
    "        # [UPDATE] ADD SCORE TO TITLE\n",
    "        # ==========================================\n",
    "        ax.set_title(f\"EXP 3: {method}\\nGlobal Avg RMS: {overall_avg_rms:.5f} (Lower is Better)\", \n",
    "                     fontsize=15, weight='bold')\n",
    "        \n",
    "        ax.set_xlabel('\\nDimension (d)', fontsize=11)\n",
    "        ax.set_ylabel('\\nSequence Length (N)', fontsize=11)\n",
    "        ax.set_zlabel('\\nRMS Error', fontsize=11)\n",
    "        \n",
    "        ax.set_zlim(z_lims)\n",
    "        \n",
    "        ax.xaxis.pane.fill = False\n",
    "        ax.yaxis.pane.fill = False\n",
    "        ax.zaxis.pane.fill = False\n",
    "        ax.grid(False)\n",
    "        \n",
    "        m = cm.ScalarMappable(cmap=cm.coolwarm)\n",
    "        m.set_array(z)\n",
    "        m.set_clim(global_z_min, global_z_max)\n",
    "        cbar = plt.colorbar(m, ax=ax, shrink=0.5, aspect=12, pad=0.1)\n",
    "        cbar.set_label('RMS Error (Shared Scale)')\n",
    "        \n",
    "        ax.view_init(elev=30, azim=-130)\n",
    "        plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# THIS PART PROVIDES ADDITIONAL VISUALIZATIONS FOR EXP 2: TRO CHARTS CORRESPOND TO EXPERIMENTS A AND B COMPARING BASELINE AND OUR METHOD.\n",
    "# ==========================================\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "def generate_aggregate_data():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING POPULATION DATA (Grid Sweep)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # A diverse mix of N and d to create a robust statistical distribution\n",
    "    n_values = [100, 250, 500, 1000, 2000, 5000, 10_000]\n",
    "    d_values = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    methods_map = {\n",
    "        \"RandNorm Gaussian\": Method_RandomGaussian,\n",
    "        \"Static Prime\": StaticPrime\n",
    "    }\n",
    "    \n",
    "    total = len(n_values) * len(d_values) * len(methods_map)\n",
    "    count = 0\n",
    "    \n",
    "    for d in d_values:\n",
    "        # Init models\n",
    "        models = {name: cls(d).to(device) for name, cls in methods_map.items()}\n",
    "        \n",
    "        for n in n_values:\n",
    "            # Skip invalid combos\n",
    "            if n <= d: continue\n",
    "            \n",
    "            wb = get_welch_bound(n, d)\n",
    "            \n",
    "            for name, model in models.items():\n",
    "                with torch.no_grad():\n",
    "                    vecs = model(n)\n",
    "                    # Calculate Max Coherence efficiently\n",
    "                    vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)\n",
    "                    gram = torch.mm(vecs, vecs.t())\n",
    "                    mask = ~torch.eye(n, device=device, dtype=torch.bool)\n",
    "                    vals = gram[mask]\n",
    "                    max_coh = vals.abs().max().item() if vals.numel() > 0 else 0.0\n",
    "                \n",
    "                # Metric 1: Optimality Ratio (Ideal = 1.0)\n",
    "                optimality = max_coh / wb if wb > 0 else 1.0\n",
    "                \n",
    "                # Metric 2: Residual / Distance (Ideal = 0.0)\n",
    "                residual = max_coh - wb\n",
    "                \n",
    "                data.append({\n",
    "                    \"Method\": name,\n",
    "                    \"N\": n,\n",
    "                    \"Dimension\": d,\n",
    "                    \"Optimality\": optimality,\n",
    "                    \"Residual\": residual\n",
    "                })\n",
    "                \n",
    "                count += 1\n",
    "                if count % 10 == 0:\n",
    "                    print(f\"\\rSimulating: {count}/{total}\", end=\"\")\n",
    "                    \n",
    "    print(\"\\nData generation complete.\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_custom_legend(palette, target_label):\n",
    "    \"\"\"\n",
    "    Manually creates legend handles to ensure they always appear.\n",
    "    \"\"\"\n",
    "    handles = []\n",
    "    for name, color in palette.items():\n",
    "        patch = mpatches.Patch(color=color, label=name, alpha=0.6)\n",
    "        handles.append(patch)\n",
    "    line = mlines.Line2D([], [], color='black', linestyle='--', \n",
    "                         linewidth=2, label=target_label)\n",
    "    handles.append(line)\n",
    "    return handles\n",
    "\n",
    "def plot_final_optimality_distribution(df):\n",
    "    \"\"\"\n",
    "    Plot 1: Optimality Ratios.\n",
    "    \"\"\"\n",
    "    df_plot = df.copy()\n",
    "    \n",
    "    # --- FIX IS HERE ---\n",
    "    # The key must match \"RandNorm Gaussian\" exactly as defined in generate_aggregate_data\n",
    "    name_map = {\n",
    "        \"RandNorm Gaussian\": \"RandNorm Gaussian (Baseline)\", \n",
    "        # \"Gaussian JL\": \"Gaussian JL\",\n",
    "        # \"Sobol\": \"Sobol (Quasi-Random)\",\n",
    "        # \"Golden Uniform\": \"Golden Uniform\",\n",
    "        \"Static Prime\": \"Static Prime (Ours)\"\n",
    "    }\n",
    "    # -------------------\n",
    "    \n",
    "    df_plot[\"Method\"] = df_plot[\"Method\"].map(name_map)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    palette = {\n",
    "        \"RandNorm Gaussian (Baseline)\": \"#eb4034\",\n",
    "        # \"Gaussian JL\": \"#95a5a6\",\n",
    "        # \"Sobol (Quasi-Random)\": \"#3498db\",\n",
    "        # \"Golden Uniform\": \"#e67e22\",\n",
    "        \"Static Prime (Ours)\": \"#02f5f5\", #\"#2ecc71\"\n",
    "    }\n",
    "    \n",
    "    sns.kdeplot(\n",
    "        data=df_plot, \n",
    "        x=\"Optimality\", \n",
    "        hue=\"Method\", \n",
    "        palette=palette, \n",
    "        fill=True, \n",
    "        alpha=0.3, \n",
    "        linewidth=2.5, \n",
    "        common_norm=False,\n",
    "        legend=False \n",
    "    )\n",
    "    \n",
    "    plt.axvline(x=1.0, color='black', linestyle='--', linewidth=2)\n",
    "    \n",
    "    handles = create_custom_legend(palette, \"Welch Bound (Ideal)\")\n",
    "    plt.legend(handles=handles, title=\"Method / Reference\", loc='upper right', frameon=True)\n",
    "    \n",
    "    plt.title(\"Distribution of Optimality Ratios\", fontsize=16, weight='bold')\n",
    "    plt.xlabel(\"Optimality Ratio (Actual / Welch Bound)\\nCloser to 1.0 is Better\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    \n",
    "    sns.despine()\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_final_residual_distribution(df):\n",
    "    \"\"\"\n",
    "    Plot 2: Residuals.\n",
    "    \"\"\"\n",
    "    df_plot = df.copy()\n",
    "    \n",
    "    # --- FIX IS HERE ---\n",
    "    name_map = {\n",
    "        \"RandNorm Gaussian\": \"RandNorm Gaussian (Baseline)\",\n",
    "        \"Static Prime\": \"Static Prime (Ours)\"\n",
    "    }\n",
    "    # -------------------\n",
    "    \n",
    "    df_plot[\"Method\"] = df_plot[\"Method\"].map(name_map)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    palette = {\n",
    "        \"RandNorm Gaussian (Baseline)\": \"#eb4034\",\n",
    "        \"Static Prime (Ours)\": \"#02f5f5\", # \"#2ecc71\"\n",
    "    }\n",
    "    \n",
    "    sns.kdeplot(\n",
    "        data=df_plot, \n",
    "        x=\"Residual\", \n",
    "        hue=\"Method\", \n",
    "        palette=palette, \n",
    "        fill=True, \n",
    "        alpha=0.3, \n",
    "        linewidth=2.5, \n",
    "        common_norm=False,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    plt.axvline(x=0.0, color='black', linestyle='--', linewidth=2)\n",
    "    \n",
    "    handles = create_custom_legend(palette, \"Zero Excess (Perfect)\")\n",
    "    plt.legend(handles=handles, title=\"Method / Reference\", loc='upper right', frameon=True)\n",
    "    \n",
    "    plt.title(\"Distribution of Excess Coherence\", fontsize=16, weight='bold')\n",
    "    plt.xlabel(\"Excess Coherence (Max Coherence - Welch Bound)\\nCloser to 0.0 is Better\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    \n",
    "    sns.despine()\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "# ==========================================\n",
    "#  MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(42) \n",
    "    # Run 3D Distribution Analysis\n",
    "    run_exp2_distribution_3d()\n",
    "    \n",
    "    # Run 3D Welch Analysis\n",
    "    run_exp3_welch_3d()\n",
    "\n",
    "    # Run 3D RMS Error Analysis\n",
    "    run_exp4_rms_error_3d()\n",
    "\n",
    "    # Use seaborn theme ########################\n",
    "    sns.set_theme(style=\"white\", context=\"talk\")\n",
    "    \n",
    "    # Generate Data\n",
    "    df_agg = generate_aggregate_data()\n",
    "    \n",
    "    # 1. Optimality Distribution\n",
    "    plot_final_optimality_distribution(df_agg)\n",
    "    \n",
    "    # 2. Residual Distribution\n",
    "    plot_final_residual_distribution(df_agg)  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
